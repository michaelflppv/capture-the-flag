package de.sep.cfp4.technicalServices.ai.mcts;

import ai.djl.ndarray.NDArray;
import ai.djl.ndarray.NDList;
import ai.djl.ndarray.NDManager;
import ai.djl.ndarray.types.Shape;
import ai.djl.training.GradientCollector;
import ai.djl.training.ParameterStore;
import ai.djl.training.loss.Loss;
import ai.djl.training.optimizer.Optimizer;

import java.io.FileInputStream;
import java.io.FileOutputStream;
import java.io.IOException;
import java.io.ObjectInputStream;
import java.io.ObjectOutputStream;
import java.nio.FloatBuffer;
import java.util.ArrayList;
import java.util.Collections;
import java.util.List;

/**
 * The ModelTraining class is responsible for training the neural network model using the MCTS algorithm.
 * It uses the self-play method to generate training data and the train method to train the model.
 * The learn method is used to train the model for a given number of iterations.

 * This class is not meant for the functionality at the moment, but for the future implementation!
 *
 * @author mfilippo (Mikhail Filippov)
 * @version 04.05.2024
 */
public class ModelTraining {
    // Model used for training
    private ArchitectureModel model;
    // Optimizer used for training
    private Optimizer optimizer;
    // Game on which the MCTS algorithm is applied
    private final MCTSClient client;
    // Hyperparameters for the MCTS algorithm
    private final Arguments arguments;
    // Monte Carlo Tree Search algorithm
    private final MonteCarloTreeSearch mcts;
    // NDManager for the MCTS algorithm
    private final NDManager manager;

    /**
     * Constructor for the ModelTraining class.
     *
     * @param model      {@link ArchitectureModel} the model used for training
     * @param optimizer  {@link Optimizer} the optimizer used for training
     * @param client     {@link MCTSClient} the game on which the MCTS algorithm is applied
     * @param arguments  {@link Arguments} map for hyperparameters of MCTS
     */
    public ModelTraining(ArchitectureModel model, Optimizer optimizer, MCTSClient client, Arguments arguments) {
        this.model = model;
        this.optimizer = optimizer;
        this.client = client;
        this.arguments = arguments;
        this.mcts = new MonteCarloTreeSearch(model, client, arguments);
        this.manager = client.getManager();
    }

    /**
     * The selfPlay method is used to generate training data by playing games against itself.
     *
     * @return List<Object[]> the training data generated by self-play
     */
    public List<MemoryElement> selfPlay() {
        // Initialize memory
        List<MemoryElement> memory = new ArrayList<>();
        // Initialize player and state
        String player = client.getOpponent(this.client.getTeamID());
        String[][] state = this.client.getInitialState();

        while (true) {
            // TODO: Delete changing perspective
            // Change perspective of the state
            String[][] neutralState = this.client.changePerspective(state, player);
            // Search for the best action to play
            float[] actionProbs = this.mcts.search(neutralState);
            // Add the state, action probabilities, and player to the memory
            memory.add(new MemoryElement(this.client.getEncodedState(neutralState), actionProbs, player, 0.0f));

            // Apply temperature to the action probabilities
            float[] temperatureActionProbs = new float[actionProbs.length];
            for (int i = 0; i < actionProbs.length; i++) {
                temperatureActionProbs[i] = (float) Math.pow(actionProbs[i], 1 / arguments.getTemperature());
            }
            double sum = 0;
            for (double prob : temperatureActionProbs) {
                sum += prob;
            }
            for (int i = 0; i < temperatureActionProbs.length; i++) {
                temperatureActionProbs[i] /= (float) sum;
            }
            int action = sampleAction(temperatureActionProbs);

            // Apply the action to the state
            state = this.client.getNextState(state, action);

            // Get the value and check if the game is terminal
            double value = this.client.getValue(action);
            boolean isTerminal = this.client.getTerminated(state, action);

            // If the game is terminal, add the training data to the memory
            if (isTerminal) {
                List<MemoryElement> returnMemory = new ArrayList<>();
                for (MemoryElement element : memory) {
                    if (element.player().equals(player)) {
                        returnMemory.add(new MemoryElement(element.memoryState(), element.actionProbs(), element.player(), (float) value));
                    } else {
                        returnMemory.add(new MemoryElement(element.memoryState(), element.actionProbs(), element.player(), this.client.getOpponentValue((int) value)));
                    }
                }
                return returnMemory;
            }

            // Change player
            player = this.client.getOpponent(player);
        }
    }

    /**
     * The sampleAction method is used to sample an action based on the probabilities.
     *
     * @param probabilities double[] the probabilities of each action
     * @return int the sampled action
     */
    private int sampleAction(float[] probabilities) {
        double cumulativeProbability = 0;
        double rand = Math.random();
        for (int i = 0; i < probabilities.length; i++) {
            cumulativeProbability += probabilities[i];
            if (rand < cumulativeProbability) {
                return i;
            }
        }
        return probabilities.length - 1;
    }

    /**
     * The train method is used to train the neural network model using the training data generated by self-play.
     *
     * @param memory List<MemoryElement> the training data generated by self-play
     */
    public void train(List<MemoryElement> memory) {
        // Shuffle the memory
        Collections.shuffle(memory);
        // Define the batch size
        int batchSize = this.arguments.getBatchSize();
        // Train the model using the training data
        for (int batchIdx = 0; batchIdx < memory.size(); batchIdx += batchSize) {
            List<MemoryElement> batch = memory.subList(batchIdx, Math.min(memory.size() - 1, batchIdx + this.arguments.getBatchSize()));

            List<float[][][]> stateList = new ArrayList<>();
            List<float[]> policyTargetsList = new ArrayList<>();
            List<Float> valueTargetsList = new ArrayList<>();

            for (MemoryElement item : batch) {
                stateList.add(item.memoryState());
                policyTargetsList.add(item.actionProbs());
                valueTargetsList.add(item.outcome());
            }

            NDArray state = createNDArray(stateList);
            NDArray policyTargets = manager.create(policyTargetsList.toArray(new float[0][]));
            NDArray valueTargets = manager.create(valueTargetsList.stream().mapToDouble(f -> f).toArray());

            // Backward pass
            try (GradientCollector gc = manager.getEngine().newGradientCollector()) {
                // Initialize the parameter store and optimizer
                ParameterStore ps = new ParameterStore(manager, false);

                // Forward pass
                NDList modelInput = new NDList(state);
                NDList modelOutput = model.forward(ps, modelInput, false);
                NDArray outPolicy = modelOutput.get(0);
                NDArray outValue = modelOutput.get(1);

                // Compute the loss
                NDArray policyLoss = Loss.softmaxCrossEntropyLoss().evaluate(new NDList(policyTargets), new NDList(outPolicy));
                NDArray valueLoss = Loss.l2Loss().evaluate(new NDList(valueTargets), new NDList(outValue));
                NDArray loss = policyLoss.add(valueLoss);

                gc.backward(loss);
                optimizer.update("paramName", outPolicy, outValue);
            }
        }

        System.out.println("Training is completed.");
    }

    public NDArray createNDArray(List<float[][][]> input) {
        int totalSize = 0;
        for (float[][][] array3D : input) {
            totalSize += array3D.length * array3D[0].length * array3D[0][0].length;
        }

        FloatBuffer buffer = FloatBuffer.allocate(totalSize);

        for (float[][][] array3D : input) {
            for (float[][] array2D : array3D) {
                for (float[] array1D : array2D) {
                    for (float value : array1D) {
                        buffer.put(value);
                    }
                }
            }
        }

        buffer.flip();

        return manager.create(buffer, new Shape(input.size(), input.get(0).length, input.get(0)[0].length, input.get(0)[0][0].length));
    }

    /**
     * The learn method is used to train the model for a given number of iterations.
     *
     * @throws IOException          if an I/O error occurs
     */
    public void learn() throws IOException {
        for (int iteration = 0; iteration < this.arguments.getNumIterations(); iteration++) {
            System.out.println("Learning Iteration: " + iteration);
            System.out.println();

            List<MemoryElement> memory = new ArrayList<>();

            // Set the model to prediction mode
            model.toPredictor();
            // Perform self-play iterations
            for (int selfPlayIteration = 0; selfPlayIteration < this.arguments.getNumSelfPlayIterations(); selfPlayIteration++) {
                System.out.println("Self-Play Iteration: " + selfPlayIteration);
                memory.addAll(selfPlay());
            }

            // Set the model to training mode
            model.toTrain();
            // Perform MCTS searches
            for (int epoch = 0; epoch < this.arguments.getNumEpochs(); epoch++) {
                System.out.println("Epoch: " + epoch);
                train(memory);
            }

            // Save model and optimizer
            saveModelAndOptimizer(iteration);
        }

        System.out.println("Learning is completed.");
    }

    /**
     * The saveModelAndOptimizer method is used to save the model and optimizer to files.
     *
     * @param iteration int the iteration number
     * @throws IOException if an I/O error occurs
     */
    private void saveModelAndOptimizer(int iteration) throws IOException {
        // Define the file paths for the model and optimizer
        String modelFilePath = "model_" + iteration + ".ser";
        String optimizerFilePath = "optimizer_" + iteration + ".ser";

        // Save the model
        try (FileOutputStream fileOut = new FileOutputStream(modelFilePath);
             ObjectOutputStream out = new ObjectOutputStream(fileOut)) {
            out.writeObject(model);

            System.out.println("Model saving is completed.");
            System.out.println("Model saved to: " + modelFilePath);
        }

        // Save the optimizer
        try (FileOutputStream fileOut = new FileOutputStream(optimizerFilePath);
             ObjectOutputStream out = new ObjectOutputStream(fileOut)) {
            out.writeObject(optimizer);

            System.out.println("Optimizer saving is completed.");
            System.out.println("Optimizer saved to: " + modelFilePath);
        }
    }

    /**
     * The loadModelAndOptimizer method is used to load the model and optimizer from files.
     *
     * @param iteration     int the iteration number
     * @throws IOException            if an I/O error occurs
     * @throws ClassNotFoundException if the class of the object cannot be found
     */
    public void loadModelAndOptimizer(int iteration) throws IOException, ClassNotFoundException {
        // Define the file paths for the model and optimizer
        String modelFilePath = "model_" + iteration + ".ser";
        String optimizerFilePath = "optimizer_" + iteration + ".ser";

        // Load the model
        try (FileInputStream fileIn = new FileInputStream(modelFilePath);
             ObjectInputStream in = new ObjectInputStream(fileIn)) {
            this.model = (ArchitectureModel) in.readObject();

            System.out.println("Model loading is completed.");
        }

        // Load the optimizer
        try (FileInputStream fileIn = new FileInputStream(optimizerFilePath);
             ObjectInputStream in = new ObjectInputStream(fileIn)) {
            this.optimizer = (Optimizer) in.readObject();

            System.out.println("Optimizer loading is completed.");
        }
    }

}
